---
layout: page
permalink: /publications/
title: Publications
---

# Under review

* <span style="font-size:20px">**Słowik A.**, Ren Y., Guo S., Mathewson K. (2020) **On Disentanglement and Compositionality in Visual Language Games**. </span>

<span style="color:green;font-size:20px"> TLDR: We study the correlation between language compositionality and disentanglement in a visual reconstruction game. We find that, counter-intuitively, the metric designed to measure compositionality in language games is negatively correlated with the positional disentanglement metric. We integrate the implementation of our beta-VAE game in the emergent communication framework to encourage research on language games with a continuous communication channel and visual input. </span>

* <span style="font-size:20px"> Lamb A., Goyal A., **Słowik A.**, Beaudoin P., Mozer M., Bengio Y. (2020) **Neural Function Modules with Sparse Arguments: A Dynamic Approach to Integrating Information across Layers**. </span>

<span style="color:green;font-size:20px"> TLDR: Feed-forward neural networks consist of a sequence of layers, in which each layer performs some processing on the information from the previous layer. A downside to this approach is that each layer (or module, as multiple modules can operate in parallel) is tasked with processing the entire hidden state, rather than a particular part of the state which is most relevant for that module. Methods which only operate on a small number of input variables are an essential part of most programming languages, and they allow for improved modularity and code re-usability.  Our proposed method, Neural Function Modules (NFM), aims to introduce the same structural capability into deep learning. When NFM is applied to a layer, it uses attention to select its input dynamically from the set of previously computed layers.  NFM is a new and generic way to compose layers in deep networks to allow for improved specialization. This newly proposed Neural Function Module (NFM) is highly flexible and we show that it can be integrated into many different types of architectures, and improve results on classification, generative modeling, and systematic generalization. </span>

* <span style="font-size:20px"> **Słowik<sup>\*</sup> A.**, Gupta<sup>\*</sup> A., Hamilton W. L., Jamnik M., Holden S. B., Pal C. (2020) **Structural Inductive Biases in Emergent Communication**. </span>

<span style="color:green;font-size:20px"> TLDR: Recent studies in emergent communication have demonstrated the limitations of developing a systematically compositional language through playing co-operative games. We investigate structural inductive biases in the setting of emergent communication by developing graph referential games. We empirically show that agents parametrized by graph neural networks develop a more compositional language compared to bag-of-words and sequence models, which allows them to systematically generalize to new combinations of familiar features. We show that the graph agents learn to understand hierarchical and relational input through communication. We will release the attached code repository upon acceptance. </span>

# Accepted 

* <span style="font-size:20px"> Gupta<sup>\*</sup> A., **Słowik<sup>\*</sup> A.**, Hamilton W. L., Jamnik M., Holden S. B., Pal C. (2020) **Analyzing Structural Priors in Multi-Agent Communication**. *Workshop on Adaptive and Learning Agents at AAMAS 2020.*  </span>

<span style="color:green;font-size:20px"> TLDR: Human language and thought are characterized by the ability to systematically generate a potentially infinite number of complex structures (e.g., sentences) from a finite set of familiar components (e.g., words). Recent works in emergent communication have discussed the propensity of artificial agents to develop a systematically compositional language through playing co-operative referential games. The degree of structure in the input data was found to affect the compositionality of the emerged communication protocols. Thus, we explore various structural priors in multi-agent communication and propose a novel graph referential game. We compare the effect of structural inductive bias (bag-of-words, sequences and graphs) on the emergence of compositional understanding of the input concepts measured by topographic similarity and generalization to unseen combinations of familiar properties. We empirically show that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. We further perform ablation studies that show the robustness of the emerged protocol in graph referential games.</span>

* <span style="font-size:20px"> **Słowik<sup>\*</sup> A.**, Gupta<sup>\*</sup> A., Hamilton W. L., Jamnik M., Holden S. B. (2020) **Towards Graph Representation Learning in Emergent Communication**. *Workshop on Reinforcement Learning in Games at AAAI 2020.*   [arxiv](https://arxiv.org/pdf/2001.09063.pdf) </span>

<span style="color:green;font-size:20px"> TLDR: Recent findings in neuroscience suggest that the human brain represents information in a geometric structure (for instance, through conceptual spaces). In order to communicate, we flatten the complex representation of entities and their attributes into a single word or a sentence. In this paper we use graph convolutional networks to support the evolution of language and cooperation in multi-agent systems. Motivated by an image-based referential game, we propose a graph referential game with varying degrees of complexity, and we provide strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. We show that the emerged communication protocol is robust, that the agents uncover the true factors of variation in the game, and that they learn to generalize beyond the samples encountered during training. </span>

* <span style="font-size:20px"> Danel T., Spurek P., Tabor J., Śmieja M., Struski Ł., **Słowik A.**, Maziarka Ł. (2019) **Spatial Graph Convolutional Networks**. *Springer Series: Communications in Computer and Information Science (CCIS), ISSN: 1865-0929.* [arXiv](https://arxiv.org/abs/1909.05310) </span>

<span style="color:green;font-size:20px"> TLDR: Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, our model outperforms state-of-the-art graph-based methods on image classification and chemical tasks.</span>

* <span style="font-size:20px"> **Słowik A.**, Mangla C., Jamnik M., Holden S. B., Paulson L. C. (2019) **Bayesian Optimisation for Heuristic Configuration in Automated Theorem Proving**. *The 6th Vampire Workshop at The 22nd International Conference on Theory and Applications of Satisfiability Testing (SAT 2019), The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020). Presented as a [talk](https://www.dropbox.com/s/f3jb7z7bqrv2b3v/bo_lisbon.pdf?dl=0) at SAT 2019 (Workshop track) and as a [poster](https://www.dropbox.com/s/h374c7zko81zrmh/BO_poster-2.pdf?dl=0) at AAAI 2020 (Student Abstract track).* [proceedings](https://easychair.org/publications/paper/K7Zd) </span>

<span style="color:green;font-size:20px"> TLDR: Modern theorem provers utilise a wide array of heuristics to control the search space explosion, thereby requiring optimisation of a large set of parameters. An exhaustive search in this multi-dimensional parameter space is intractable in most cases, yet the performance of the provers is highly dependent on the parameter assignment. In this work, we introduce a principled probabilistic framework for heuristic optimisation in theorem provers. We present results using a heuristic for premise selection and the Archive of Formal Proofs (AFP) as a case study. </span>

* <span style="font-size:20px"> Antoniou A., **Słowik A.**, Crowley E. J., Storkey A. J. (2018) **Dilated DenseNets for Relational Reasoning**. *Women in Machine Learning Workshop at NeurIPS 2018 (WiML 2018), Theoretical Foundations of Machine Learning (TFML 2019), Transylvanian Machine Learning Summer School (TMLSS)*. [arXiv](https://arxiv.org/pdf/1811.00410.pdf) [slides](https://www.dropbox.com/s/agauohqy2gw436n/AI_lunch.pdf?dl=0) [poster](https://www.dropbox.com/s/8wucj3reyj2anlp/dil_poster.pdf?dl=0) </span>

<span style="color:green;font-size:20px"> TLDR: Despite their impressive performance in many tasks, deep neural networks often struggle at relational reasoning. This has recently been remedied with the introduction of a plug-in relational module that considers relations between pairs of objects. Unfortunately, this is combinatorially expensive. In this extended abstract, we show that a DenseNet incorporating dilated convolutions excels at relational reasoning on the Sort-of-CLEVR dataset, allowing us to forgo this relational module and its associated expense. </span>

* <span style="font-size:20px"> **Słowik A.**, Czarnecki W. (2016) **Random projections in Extreme Learning Machines**. *Women in Machine Learning Workshop at NIPS 2016 (WiML 2016)*. [slides](https://www.dropbox.com/s/9lnlz8ny3qch7ej/Extreme_Learning_Machines-2.pdf?dl=0) </span>

<span style="color:green;font-size:20px"> TLDR: Despite growing research in random neural networks the problem of choosing weights distribution has been ignored for a long time and sampling from Gaussian noise remains the state-of-the-art. My BSc thesis compares three groups of random projections: data-independent probability distributions, semi-supervised sampling from clustered training examples, and supervised sampling in the region of the decision boundary. I analyse the effect of the distribution choice on the performance of Extreme Learning Machines, the most popular RNN model introduced by Guang Bin-Huang. I report the results using a suite of 5 classification and 5 regression datasets. </span>

    
    



